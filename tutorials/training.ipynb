{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models in PyTorch\n",
    "\n",
    "This notebook is a tutorial accompying the manuscript \"Perspectives: Comparison of Deep Learning Based Segmentation Models on Typical Biophysics and Biomedical Data\" by JS Bryan IV, M Tavakoli, and S Presse. In this tutorial, we will learn the basics of training models in PyTorch.\n",
    "\n",
    "**Before reading this tutorial, make sure you have properly installed PyTorch and downloaded the data as explained in this repository's README.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will train a simple convolutional neural network (CNN) to train models in PyTorch. We will specifically train a simple model to segment the Chinese characters from the images in the dataset. The specific aim of this tutorial is the explain the training function defined in the `training.py` of this repository.\n",
    "\n",
    "### Importing libraries\n",
    "\n",
    "Before we start, let's import the necessary libraries. To get started here are the libraries we will use:\n",
    "\n",
    "* os: To handle file paths.\n",
    "* torch: To access PyTorch functionalities.\n",
    "* torch.nn: To access PyTorch's neural network functionalities.\n",
    "* torch.nn.functional: To access PyTorch's functional neural network functionalities.\n",
    "* matplotlib.pyplot: To plot images and graphs.\n",
    "* torch.utils.data.DataLoader: To load data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "We will define a simple CNN model with two convolutional layers and two fully connected layers. The model will be defined as a class `SimpleCNN` that inherits from `nn.Module`. For more information on defining models in PyTorch, please check out our tutorial on models in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parts of training a model\n",
    "\n",
    "When we creat a model in PyTorch, the weights are initialized to random values. The goal of training is to adjust these weights so that the model can make accurate predictions. In order to train the model we need to define the following components:\n",
    "\n",
    "1) **Dataset**: The dataset that we will use to train the model. We will use the `ChineseCharacters` dataset in this tutorial.\n",
    "2) **Model**: The neural network model that we want to train.\n",
    "3) **Loss Function**: The loss function that we will use to calculate the error between the predicted and actual values.\n",
    "4) **Optimizer**: The optimizer that we will use to adjust the weights of the model.\n",
    "\n",
    "Let us go through each of these components in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will use the `ChineseCharacters` dataset to train the model. The dataset contains images of Chinese characters along with their segmentation masks. We will import the dataset from our `data/` directory, instantiate the dataset, and then split it into training, validation, and testing sets. The training set will be used to train the model, the validation set will be used to stop training when the model starts overfitting, and the testing set will be used to evaluate the model's performance. We will also create a `DataLoader` for each set to load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom Chinese Characters dataset\n",
    "basedir = os.path.dirname(os.getcwd())\n",
    "datadir = os.path.join(basedir, 'data')\n",
    "sys.path.append(datadir)\n",
    "from letters import ChineseCharacters\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ChineseCharacters()\n",
    "\n",
    "# Split the dataset\n",
    "n_train = int(0.6 * len(dataset))\n",
    "n_val = int(0.2 * len(dataset))\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# Set up data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what the data looks like, we will plot a few images and their corresponding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first image and mask of the next batch\n",
    "img, mask = next(iter(train_loader))\n",
    "img = img[0]\n",
    "mask = mask[0]\n",
    "\n",
    "# Visualize the first image and mask\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img.numpy().transpose(1, 2, 0))  # Transpose the image tensor to (H, W, C)\n",
    "ax[0].set_title('Image')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(mask.numpy())  # The mask is a single channel tensor\n",
    "ax[1].set_title('Mask')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will define a simple CNN model with two convolutional layers and two fully connected layers. The model will be defined as a class `SimpleCNN` that inherits from `nn.Module`. For more information on defining models in PyTorch, please check out our tutorial on models in PyTorch.\n",
    "\n",
    "We will define the model and then instantiate it. Notice that we have 1 input channel (since the images are grayscale) and 2 output channels (one representing the background and the other representing the Chinese characters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ConvolutionalNet class\n",
    "class ConvolutionalNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_features=8, n_layers=2):\n",
    "        super(ConvolutionalNet, self).__init__()\n",
    "\n",
    "        # Set up attributes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_features = n_features\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Set up input block\n",
    "        self.input_block = nn.Sequential(\n",
    "            nn.GroupNorm(1, in_channels, affine=False),  # Normalize input\n",
    "            nn.Conv2d(in_channels, n_features, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        # Set up layers\n",
    "        self.conv_block = nn.ModuleList()\n",
    "        for _ in range(self.n_layers):\n",
    "            self.conv_block.append(nn.Sequential(\n",
    "                nn.Conv2d(n_features, n_features, kernel_size=3, padding=1),\n",
    "                nn.InstanceNorm2d(n_features),\n",
    "                nn.ReLU(),\n",
    "            ))\n",
    "\n",
    "        # Set up output block\n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Conv2d(self.n_features, out_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "\n",
    "        # Input block\n",
    "        x = self.input_block(x)\n",
    "\n",
    "        # Convolutional block\n",
    "        for layer in self.conv_block:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Output block\n",
    "        x = self.output_block(x)\n",
    "\n",
    "        # Return\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "model = ConvolutionalNet(1, 2, n_features=8, n_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is instantiated, but not yet trained. Let us look at the output of the model before training so we can compare it to the output after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next batch\n",
    "img, mask = next(iter(train_loader))\n",
    "\n",
    "# Forward pass\n",
    "output = model(img)\n",
    "prediction = output.argmax(dim=1).squeeze()\n",
    "\n",
    "# Visualize the first image and mask\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].set_title('Image')\n",
    "ax[0].axis('off')\n",
    "ax[0].imshow(img[0].numpy().transpose(1, 2, 0))  # Transpose the image tensor to (H, W, C)\n",
    "ax[1].set_title('Mask')\n",
    "ax[1].axis('off')\n",
    "ax[1].imshow(mask[0].numpy())  # The mask is a single channel tensor\n",
    "ax[2].set_title('Prediction')\n",
    "ax[2].axis('off')\n",
    "ax[2].imshow(prediction[0].detach().numpy())  # The output is a two-channel tensor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the ouput prediction will be random since the model has not been trained yet. Notice that if you rerun the cell after training the model, the output will be different. Try not to rerun the cell after training the model in order to preserve the random output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function is used to calculate the error between the predicted and actual values. Our model takes in a noisy image of a Chinese Character then output the logits of two classes, one for the background and one for the character. We will use the `CrossEntropyLoss` function to calculate the loss. For more information on loss functions in PyTorch, please check out our tutorial on loss functions in PyTorch.\n",
    "\n",
    "We will also add a regularization term to the loss to prevent overfitting. The regularization term is the sum of the squares of the weights of the model divided by the number of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up regularization\n",
    "def regularization(model):\n",
    "    return (\n",
    "        sum(p.pow(2.0).sum() for p in model.parameters()) \n",
    "        / sum(p.numel() for p in model.parameters())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "The optimizer is used to adjust the weights of the model based on the loss calculated by the loss function at each iteration of the training process. We will use the `Adam` optimizer with a learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Now that we have set up the model, the dataset, the loss function, and the optimizer, we can define the training loop. The training loop consists of the following steps, which are run every iteration of the training process:\n",
    "\n",
    "1) **Get the data**: Get a batch of data from the training set. This is done by iterating over the `DataLoader`.\n",
    "2) **Zero the gradients**: Zero the gradients of the model so that they do not accumulate. This is done by calling `optimizer.zero_grad()`.\n",
    "3) **Forward pass**: Pass the data through the model to get the predicted values. This is done by calling the model with the data as input, `model(data)`.\n",
    "4) **Calculate the loss**: Calculate the loss using the predicted values and the actual values. This is done by calling the loss function with the predicted values and the actual values, `loss(pred, target)`.\n",
    "5) **Backward pass**: Backpropagate the loss to adjust the weights of the model. This is done by calculating the gradients of the loss with respect to the weights of the model with `loss.backward()`.\n",
    "6) **Update the weights**: Update the weights of the model using the optimizer. This is done by calling `optimizer.step()`.\n",
    "\n",
    "Additionally, at the end of each epoch we will test the model on the validation set, save the model if it performs better than the previous best model, and print the loss and accuracy of the model on the training and validation sets.\n",
    "\n",
    "We package the training loop into a function called `train_model` that takes in the model, the training and validation datasets, the loss function, the optimizer, and the number of epochs to train for. The function will train the model for the specified number of epochs and return the trained model.\n",
    "\n",
    "Not that for tutorial purposes, we will only train for 3 epochs so that the training process is faster. In practice, you would train for many more epochs to get a well-trained model.\n",
    "\n",
    "**The following cell may take a few minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_model(model, train_loader, val_loader, n_epochs=100):\n",
    "    print('Training model...')\n",
    "\n",
    "    # Best loss\n",
    "    best_loss = float('inf')\n",
    "    best_loss_path = 'best_model.pth'\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop over training data\n",
    "        for img, mask in train_loader:  # Step 1) Get data\n",
    "\n",
    "            # Step 2) Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 3) Forward pass\n",
    "            output = model(img)\n",
    "\n",
    "            # Step 4) Compute loss\n",
    "            loss = (\n",
    "                criterion(output, mask)\n",
    "                + regularization(model)\n",
    "            )\n",
    "\n",
    "            # Step 5) Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 6) Update model\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, mask in val_loader:\n",
    "                output = model(img)\n",
    "                val_loss += criterion(output, mask).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Validation loss: {val_loss}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss <= best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_loss_path)\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(best_loss_path))\n",
    "    os.remove(best_loss_path)\n",
    "\n",
    "    # Return best model\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, train_loader, val_loader, n_epochs=3)  # Only train for 3 epochs for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, let us visualize the output of the model on a sample to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next batch\n",
    "img, mask = next(iter(train_loader))\n",
    "\n",
    "# Forward pass\n",
    "output = model(img)\n",
    "prediction = output.argmax(dim=1).squeeze()\n",
    "\n",
    "# Visualize the first image and mask\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].set_title('Image')\n",
    "ax[0].axis('off')\n",
    "ax[0].imshow(img[0].numpy().transpose(1, 2, 0))  # Transpose the image tensor to (H, W, C)\n",
    "ax[1].set_title('Mask')\n",
    "ax[1].axis('off')\n",
    "ax[1].imshow(mask[0].numpy())  # The mask is a single channel tensor\n",
    "ax[2].set_title('Prediction')\n",
    "ax[2].axis('off')\n",
    "ax[2].imshow(prediction[0].detach().numpy())  # The output is a two-channel tensor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output after training should more closely match the target masks than the output before training. Keep in mind that the output may not be perfect, since we are using a rather simple model with few parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial we learned how to train a simple CNN model in PyTorch. We defined the dataset, model, loss function, and optimizer, and then trained the model using a training loop. We also visualized the output of the model before and after training to see how well it performs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
