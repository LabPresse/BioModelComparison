{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors in PyTorch\n",
    "\n",
    "This notebook is a tutorial accompying the manuscript \"Perspectives: Comparison of Deep Learning Based Segmentation Models on Typical Biophysics and Biomedical Data\" by JS Bryan IV, M Tavakoli, and S Presse. In this tutorial, we will learn the basics of using tensors in PyTorch.\n",
    "\n",
    "**Before reading this tutorial, make sure you have properly installed PyTorch and downloaded the data as explained in this repository's README.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to this tutorial on PyTorch tensors! This guide will walk you through the essential concepts of tensors in PyTorch, covering everything from creating tensors and performing basic operations to using tensors gradients. We will not dive deep into any subject, instead just focussing the bare essentials needed to understand the manuscript.\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. They are similar to NumPy arrays, but with additional features that make them suitable for deep learning. Tensors can be used to represent scalars, vectors, matrices, and higher-dimensional arrays. They also support automatic differentiation, which is essential for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "To get started, we need to import `torch`, the PyTorch library. We will also import `numpy` for comparison. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a tensor? (Basically a fancy numpy array)\n",
    "\n",
    "A tensor is essentially just some object that contains numbers in a multi-dimensional grid. At each point in the grid, there is a single number, which is the value of the tensor at that point. In the same way that a vector is a list of numbers and a matrix is a 2D grid of numbers, we can have tensors with three, four, or more dimensions. We can also have tensors with 0 dimensions, which is just a single number, also known as a scalar.\n",
    "\n",
    "For those familiar with NumPy, tensors are basically the same thing as NumPy arrays. In fact, you can often use NumPy functions on tensors and vice versa. However, tensors have some additional features that make them more suitable for deep learning, like GPU acceleration and automatic differentiation. We will cover these features later in the tutorial. For now let's just focus on the basics.\n",
    "\n",
    "Let us cover a few tensor basics:\n",
    "\n",
    "- Creating tensors\n",
    "- Tensor attributes\n",
    "- Tensor operations\n",
    "- Tensor slicing and indexing\n",
    "- Tensor broadcasting\n",
    "- Tensor reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "\n",
    "There are a few ways to create tensors in PyTorch. The simplest way is to use the `torch.tensor` function, which takes a list (or array) of numbers and creates a tensor with the same shape. We can also initialize tensors with zeros, ones, or random values. We can also create a tensor from a numpy array. Let's see some examples, comparing tensors with NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "np_zeros = np.zeros((2, 2))\n",
    "np_rand = np.random.randn(2, 2)\n",
    "\n",
    "# PyTorch tensor\n",
    "torch_tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "torch_zeros = torch.zeros((2, 2))\n",
    "torch_rand = torch.randn(2, 2)\n",
    "torch_from_numpy = torch.tensor(np_array)\n",
    "\n",
    "# Print\n",
    "print(f\"NumPy array: {np_array}\")\n",
    "print(f\"Torch tensory: {torch_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we can tensors have have 0, 1, 2, 3, or more dimensions. Here are some examples of tensors with different dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensors\n",
    "tensor_0D = torch.tensor(4)\n",
    "tensor_1D = torch.tensor([1, 2, 3])\n",
    "tensor_2D = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_3D = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# Print the tensors\n",
    "print(f\"0D tensor: {tensor_0D}\")\n",
    "print(f\"1D tensor: {tensor_1D}\")\n",
    "print(f\"2D tensor: {tensor_2D}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Attributes\n",
    "\n",
    "Tensors have several attributes that describe their shape, data type, and device (CPU or GPU). The most important attributes are:\n",
    "\n",
    "- `shape`: the dimensions of the tensor.\n",
    "- `dtype`: the data type of the tensor (e.g., float32, int64).\n",
    "- `device`: the device (CPU or GPU) where the tensor is stored.\n",
    "\n",
    "Let's see how to access these attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Print the tensor attributes\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print(f\"Data type: {tensor.dtype}\")\n",
    "print(f\"Device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations\n",
    "\n",
    "Tensors support many of the same operations as NumPy arrays, such as addition, subtraction, multiplication, and division. We can also apply functions like `sin`, `cos`, `exp`, etc., to tensors. These operations are performed element-wise, meaning they are applied to each element of the tensor individually. Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor and a numpy array\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "array = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Addition\n",
    "tensor = tensor + 1\n",
    "array = array + 1\n",
    "\n",
    "# Multiplication\n",
    "tensor = tensor * 2\n",
    "array = array * 2\n",
    "\n",
    "# Cosine\n",
    "tensor = torch.cos(tensor)  # Notice we are using torch.cos\n",
    "array = np.cos(array)       # Notice we are using np.cos\n",
    "\n",
    "# Print the tensor and the array\n",
    "print(f\"Tensor: {tensor}\")\n",
    "print(f\"Array: {array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing and Indexing Tensors\n",
    "\n",
    "Remember that tensors are multi-dimensional grids of numbers. We can access individual elements, rows, columns, or sub-tensors using slicing and indexing. The syntax is similar to NumPy arrays, however, when we are accessing a single element, we need to use the `item` method to get the actual value as a Python number, otherwise the output will be a tensor. Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array and tensor\n",
    "array = np.array([[1, 2], [3, 4]])\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Get first element (indexing)\n",
    "print(f\"Array first element: {array[0, 0]} (type={type(array[0, 0])})\")\n",
    "print(f\"Tensor first element: {tensor[0, 0]} (type={type(tensor[0, 0])})\")\n",
    "print(f\"Tensor first element (with item()): {tensor[0, 0].item()} (type={type(tensor[0, 0].item())})\")\n",
    "\n",
    "# Slice first column\n",
    "print(f\"Array first column: {array[:, 0]}\")\n",
    "print(f\"Tensor first column: {tensor[:, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Broadcasting\n",
    "\n",
    "Broadcasting is a powerful mechanism that allows PyTorch to work with tensors of different shapes in arithmetic operations. Frequently we have a smaller tensor and a larger tensor, and we want to use the smaller tensor multiple times to perform some operation on the larger tensor. For example, adding a column vector to a matrix.\n",
    "\n",
    "Let's see a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors and arrays\n",
    "torch_vec = torch.tensor([1, 2])\n",
    "torch_mtx = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Add vectors and matrices\n",
    "print(f\"Vector: {torch_vec}\")\n",
    "print(f\"Matrix: {torch_mtx}\")\n",
    "print(f\"Sum: {torch_vec + torch_mtx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that are sticking to simple vector and matrix operations in our example, but broadcasting can be applied to tensors with any number of dimensions. There are some rules for broadcasting two tensors of different dimensions together, which we won't cover here, but you can find more information in the [official documentation](https://pytorch.org/docs/stable/notes/broadcasting.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Tensors\n",
    "\n",
    "Reshaping is the process of changing the shape of a tensor. We might want to do this if we want to feed the tensor into a neural network or perform some arithmetic operation. The only requirement is that the size of the reshaped tensor remains the same as the original tensor. Reshaping is a common operation in deep learning, as we often need to manipulate the shape of tensors to match the input or output shape of a neural network.\n",
    "\n",
    "We can use `view` to reshape a tensor. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor\n",
    "tensor = torch.tensor([[1, 2, 3, 4, 5, 6]])\n",
    "\n",
    "# Reshape the tensor\n",
    "print(f\"Original tensor: {tensor}\")\n",
    "print(f\"Tensor A: {tensor.view(2, 3)}\")\n",
    "print(f\"Tensor B: {tensor.view(3, 2, 1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "Now that we have covered the basics of tensors, let's move on to a more advanced topic: gradients. Gradients are essential for training neural networks, as they allow us to determine how to update the weights of the network to minimize the loss function. PyTorch provides automatic differentiation, which means that it can automatically compute the gradients of tensors with respect to some scalar value.\n",
    "\n",
    "**Note: Tensor gradients are an incredibly important concept in PyTorch, however, in practice, most of the work that uses gradients is done automatically behind the scenes by PyTorch. You don't need to worry too much about the details of how gradients are used or computed, but it's good to have a basic understanding of what they are and how they work. The goal of this section is to provide an overview of gradients in PyTorch, but we won't go into too much detail.**\n",
    "\n",
    "Every tensor has a field called `grad`, which stores the gradient of the tensor with respect to some scalar value. By default, the `grad` field is `None`, which means that the tensor does not require gradients. If we want to compute gradients, we need to set the `requires_grad` attribute of the tensor to `True`. This tells PyTorch to track the operations on the tensor and compute the gradients with respect to some scalar value. Notice that we can only compute gradients for floating-point tensors (as opposed to integer tensors).\n",
    "\n",
    "As we perform operations on a tensor with `requires_grad=True`, the tensor will \"remember\" what operations were performed on it. After we are done with the operations, we can call the `backward` method on the result tensor to compute the gradient of the final result with respect to the original tensor.\n",
    "\n",
    "For example if we start with a tensor `x` and apply some function `y=f(x)`, we can compute the gradient of `y` with respect to `x` using the `backward` method. The gradient will be stored in the `grad` attribute of `x`. We can then access the gradient using the `grad` attribute of `x`.\n",
    "\n",
    "Notice what happens to a tensor when we set `requires_grad=True` and perform some operations on it, then call `backward` on the result. This will compute the gradients of the result with respect to the original tensor. We can access the gradients using the `grad` attribute of the original tensor. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Print original tensor gradient\n",
    "print(f\"Original tensor grad: {x.grad}\")\n",
    "\n",
    "# Perform some operations\n",
    "y = torch.cos(2.0*x + 1)\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "# Print new tensor gradients\n",
    "print(f\"New tensor grad: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor gradients are an extremely important part of training deep learning models. They allow us to compute the gradients of the loss function with respect to the model parameters, which we can then use to update the parameters using gradient descent.\n",
    "\n",
    "In practice, we very rarely access the gradients directly, as PyTorch provides optimizers that handle the gradient updates for us. However, it is important to understand how gradients work, as they are the foundation of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we covered the basics of tensors in PyTorch. We learned how to create tensors, perform basic operations, access attributes, and compute gradients. Tensors are the fundamental data structure in PyTorch, and understanding how to work with them is essential for building and training deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
