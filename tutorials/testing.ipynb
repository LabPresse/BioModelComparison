{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models in PyTorch\n",
    "\n",
    "This notebook is a tutorial accompying the manuscript \"Perspectives: Comparison of Deep Learning Based Segmentation Models on Typical Biophysics and Biomedical Data\" by JS Bryan IV, M Tavakoli, and S Presse. In this tutorial, we will learn the basics of evaluating deep learning models in PyTorch.\n",
    "\n",
    "**Before reading this tutorial, make sure you have properly installed PyTorch and downloaded the data as explained in this repository's README.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the tutorial on evaluating models in PyTorch! In this tutorial, we will learn how to evaluate models in PyTorch using the `torch` and `torch.nn` modules. The specific aim of this tutorial is the explain the test metrics used in the accompanying manuscript, which can be found in the `testing.py` file of this repository.\n",
    "\n",
    "Evaluation of models is a crucial step in the machine learning pipeline, as it allows us to understand how well our models are performing and select the best model for our specific task.\n",
    "\n",
    "### Importing libraries\n",
    "\n",
    "Before we start, let's import the necessary libraries. To get started here are the libraries we will use:\n",
    "\n",
    "* os: To handle file paths.\n",
    "* torch: To access PyTorch functionalities.\n",
    "* torch.nn: To access PyTorch's neural network functionalities.\n",
    "* torch.nn.functional: To access PyTorch's functional neural network functionalities.\n",
    "* matplotlib.pyplot: To plot images and graphs.\n",
    "* torch.utils.data.DataLoader: To load data in batches.\n",
    "* sklearn.metrics.confusion_matrix: To compute confusion matrices (the table of true and false positives and negatives).\n",
    "* sklearn.metrics.roc_curve: To compute ROC (Receiver Operating Characteristic) curves.\n",
    "* sklearn.metrics.roc_auc_score: To compute the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use our custom CNN model and ChineseCharacter datasets, which we import from other files in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom Chinese Characters dataset\n",
    "basedir = os.path.dirname(os.getcwd())\n",
    "datadir = os.path.join(basedir, 'data')\n",
    "sys.path.append(datadir)\n",
    "from letters import ChineseCharacters\n",
    "\n",
    "# Import custom CNN model\n",
    "modeldir = os.path.join(basedir, 'models')\n",
    "sys.path.append(modeldir)\n",
    "from conv import ConvolutionalNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will breifly train a model to evaluate in this tutorial, however we will not go into detail about the training process. For more information on training models in PyTorch, please refer to the training tutorial in this repository.\n",
    "\n",
    "As a breif overview, remember that we split the data into training, validation, and testing sets. We then train the model on the training set, validate the model on the validation set.\n",
    "\n",
    "**The following cell may take a few minutes to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = ChineseCharacters()\n",
    "\n",
    "# Split the dataset\n",
    "n_train = int(0.6 * len(dataset))\n",
    "n_val = int(0.2 * len(dataset))\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# Set up data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "in_features = 1\n",
    "out_features = 2\n",
    "n_features = 8\n",
    "n_layers = 2\n",
    "model = ConvolutionalNet(in_features, out_features, n_features=n_features, n_layers=n_layers)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set up loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up regularization\n",
    "def regularization(model):\n",
    "    return (\n",
    "        sum(p.pow(2.0).sum() for p in model.parameters()) \n",
    "        / sum(p.numel() for p in model.parameters())\n",
    "    )\n",
    "\n",
    "### TRAINING LOOP ###\n",
    "\n",
    "# Best loss\n",
    "best_loss = float('inf')\n",
    "best_loss_path = 'best_model.pth'\n",
    "\n",
    "# Loop over epochs\n",
    "n_epochs = 3  # Set to 3 for faster training\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "\n",
    "    # Train model\n",
    "    model.train()\n",
    "    for img, mask in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        loss = (\n",
    "            criterion(output, mask)\n",
    "            + regularization(model)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, mask in val_loader:\n",
    "            output = model(img)\n",
    "            val_loss += criterion(output, mask).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Validation loss: {val_loss}')\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss <= best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_loss_path)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_loss_path))\n",
    "os.remove(best_loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training the model, we evaluate the model on the testing set. In this tutorial, we will evaluate the model using the following metrics:\n",
    "\n",
    "* Accuracy: The percentage of correctly classified samples.\n",
    "* Specificity: The percentage of correctly classified negative samples.\n",
    "* Sensitivity: The percentage of correctly classified positive samples.\n",
    "* AUC: The area under the receiver operating characteristic curve (explained later).\n",
    "\n",
    "To calculate Accuracy, Specificity, and Sensitivity we apply the model to the testing set and compare the predicted labels to the true labels. We then calculate the number of true positives, true negatives, false positives, and false negatives to calculate the metrics.\n",
    "\n",
    "**The next cell may take a few minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to test set\n",
    "model.eval()\n",
    "TP = 0  # True positives\n",
    "FP = 0  # False positives\n",
    "TN = 0  # True negatives\n",
    "FN = 0  # False negatives\n",
    "with torch.no_grad():\n",
    "    for img, mask in test_loader:\n",
    "        output = model(img)\n",
    "        pred = output.argmax(dim=1)\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            mask.numpy().flatten(), pred.numpy().flatten()\n",
    "        ).ravel()\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        TN += tn\n",
    "        FN += fn\n",
    "\n",
    "# Calcualte accuracy, sensitivity, and specificity\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "print(f'Specificity: {specificity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to plot the ROC curve and calculate the AUC score. The ROC curve is a plot of the true positive rate (sensitivity) against the false positive rate (1-specificity) for different threshold values. We can calculate the false positive rate, true positive rate, and threshold values using the `roc_curve` function from the `sklearn.metrics` module. The AUC score is the area under the ROC curve, which ranges from 0 to 1. An AUC score of 0.5 indicates a random classifier, while an AUC score of 1 indicates a perfect classifier. We can calculate the AUC score using the `roc_auc_score` function from the `sklearn.metrics` module.\n",
    "\n",
    "**The next cell may take a few minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate false positive rate and true positive rate\n",
    "true = []\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for img, mask in test_loader:\n",
    "        output = model(img)\n",
    "        true.extend(mask.numpy().flatten())\n",
    "        pred.extend(output[:, 1].numpy().flatten())\n",
    "\n",
    "# Sort ROC curve so that fpr is increasing\n",
    "fpr, tpr, _ = roc_curve(true, pred)\n",
    "auc = roc_auc_score(true, pred)\n",
    "\n",
    "# Plot ROC curve\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows a simple implementation of plotting and calculating the ROC curve and AUC score. Notice that in the `testing.py` function, we add some complexity to handle large datasets by randomly subsampling the data after evaluation. This is because calculating the ROC curve and AUC score can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to evaluate models in PyTorch using the `torch` and `torch.nn` modules. We also learned how to calculate the Accuracy, Specificity, Sensitivity, ROC curve, and AUC score. These metrics are crucial for evaluating the performance of our models and selecting the best model for our specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
